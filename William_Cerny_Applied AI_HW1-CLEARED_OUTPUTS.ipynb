{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Intro to Chemoinformatics / QSAR Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** William Cerny <br/>\n",
    "**Class:** MENG 25160 <br/>\n",
    "**Due Date for Assignment: 2/6/2022** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General Utility Packages\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### File/IO Packages\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "### Machine Learning Packages/Modules/Functions\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "## Chemistry and Materials Packages\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit import Chem \n",
    "from rdkit.Chem import AllChem  \n",
    "from rdkit import DataStructs\n",
    "\n",
    "### Timing\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Plotting Style\n",
    "plt.rc('axes', linewidth = 2)\n",
    "\n",
    "## Bad Practice\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the first half of this notebook, we explore the ability of Lasso and Random Forest models to predict the bandgap for molecules given information about their molecular structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I note two additional things about the below:** <br/> (1) I do not strictly use the suggested number of rows for some of the parts; I chose a reasonable amount for all steps that produced robust results given a reasonable runtime, and (2) I gratefully acknowledge the code from the lecture notebooks; while I do not cite these individually when they show up below, code snippets directly from those notebooks are heavily represented in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries Part 1: Loading in Data and Calculating Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loads in a JSON and computes descriptors.\n",
    "\n",
    "start_from_json = True\n",
    "\n",
    "if start_from_json: \n",
    "    print('Assuming that features have not been pre-computed via Mordred. Loading JSON file...')\n",
    "    data_path = './datasets/qm9.json.gz'\n",
    "    df = pd.read_json(data_path, lines=True).sample(7000) ## slighly smaller than 10k to save runtime\n",
    "    print('Dataset is %d rows in total and has %d columns before rdkit'%(df.shape[0],df.shape[1]))\n",
    "    df['mol'] = df['smiles_0'].apply(Chem.MolFromSmiles)\n",
    "    calc = Calculator(descriptors, ignore_3D=True)\n",
    "    desc = calc.pandas(df['mol'])\n",
    "    print('Dataset is %d rows in total and has %d columns after applying rdkit'%(desc.shape[0],desc.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries Part 2: Removing Invalid and Useless Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove columns with no variation across all rows\n",
    "zero_variation = [c for c, s in desc.std().items() if s == 0]\n",
    "print(f'Original shape before dropping columns with no variation: {desc.shape}')\n",
    "desc.drop(columns=zero_variation, inplace=True)\n",
    "print(f'New shape: {desc.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first 5 rows of the newly-instantiated dataframe.\n",
    "desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force columns to be numerical values\n",
    "for c in desc.columns:\n",
    "    desc[c] = pd.to_numeric(desc[c], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect whether any columns have empty/NaN/invalidand drop appropriately.\n",
    "missing_values = desc.isnull().any()\n",
    "desc = desc.loc[:, ~missing_values] \n",
    "print('The dataset had %d columns with invalid entries. They have now been removed.'%missing_values.sum())\n",
    "print(f'New shape: {desc.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.1: Comparing ML Models for Bandgap Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I create a pipeline for each of the three models as a quick demo. I then compare them over the same test/training data in 1.1d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1a: LassoCV Regression with 8 Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Lasso Model with 8-feature PCA and standard scaling\n",
    "pca_8feature_model = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('pca', PCA(n_components=8)),\n",
    "    ('lasso', LassoCV())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "train_data, test_data, train_desc, test_desc = train_test_split(df, desc, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model to training data and then test it on a set of descriptors\n",
    "test_data['8-ncomp-model-pred'] = pca_8feature_model.fit(train_desc, train_data['bandgap']).predict(test_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print an estimate of the MAE for our predictons from the previous cell\n",
    "mean_absolute_error(y_true = test_data['bandgap'], y_pred = test_data['8-ncomp-model-pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1b: Lasso Regression without PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model and the next one, the steps are identical, so I will refer the reader to the comments in 1.1a to understand what is going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pca_model = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('lasso', LassoCV())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pca-free-lasso-model-pred'] = no_pca_model.fit(train_desc, train_data['bandgap']).predict(test_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true = test_data['bandgap'], y_pred = test_data['pca-free-lasso-model-pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1c: Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest regression model with default everything\n",
    "rfmodel = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['rfmodel-pred'] = rfmodel.fit(train_desc, train_data['bandgap']).predict(test_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true = test_data['bandgap'], y_pred = test_data['rfmodel-pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1d: Putting it all together: testing the 3 models' MAE as a function of training size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I considered saving the results in a dataframe here but figured that was a bit overkill since we are only interested in (3 * 3) = 9 total numbers for sake of our model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sizes = np.array([10,100,1000,5000]) # Converts number of rows to fractions needed by train_test_split\n",
    "\n",
    "mae_values = []\n",
    "\n",
    "for trainsize in tqdm(train_sizes): ## iterate through various training sizes and run each model.\n",
    "    train_data, test_data, train_desc, test_desc = train_test_split(df, desc, train_size= trainsize)\n",
    "    \n",
    "    pca8_lasso_predictions = pca_8feature_model.fit(train_desc, train_data['bandgap']).predict(test_desc)\n",
    "    nopca_lasso_predictions = no_pca_model.fit(train_desc, train_data['bandgap']).predict(test_desc)\n",
    "    rf_predictions = rfmodel.fit(train_desc, train_data['bandgap']).predict(test_desc)\n",
    "    \n",
    "    # Calc MAE for each model and store results\n",
    "    pca8_mae = mean_absolute_error(y_true = test_data['bandgap'], y_pred = pca8_lasso_predictions)\n",
    "    no_pca_mae = mean_absolute_error(y_true = test_data['bandgap'], y_pred = nopca_lasso_predictions)\n",
    "    rf_mae = mean_absolute_error(y_true = test_data['bandgap'], y_pred = rf_predictions)\n",
    "    mae_values.append([pca8_mae, no_pca_mae, rf_mae])\n",
    "    \n",
    "    # save memory\n",
    "    del train_data, test_data, train_desc, test_desc\n",
    "    \n",
    "mae_values = np.asarray(mae_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.plot(train_sizes, mae_values[:,0], label = 'Lasso with PCA', color = 'mediumblue', lw = 2, marker = 'o')\n",
    "plt.plot(train_sizes, mae_values[:,1], label = 'Lasso w/o PCA', color = 'firebrick', lw = 2, marker = 'o')\n",
    "plt.plot(train_sizes, mae_values[:,2], label = 'RF', color = 'orange', lw = 2, marker = 'o')\n",
    "\n",
    "\n",
    "plt.ylabel('Mean Absolute Error', fontsize = 15); plt.yticks(fontsize = 13.5)\n",
    "plt.xlabel('Training Size', fontsize = 15); plt.xticks(fontsize = 13.5)\n",
    "plt.xscale('log'); plt.yscale('log')\n",
    "plt.legend(fontsize = 13.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broad Interpretation**: the models appear to be effectively equivalent in terms of MAE for their predictions for the small training size case (10). At higher training sizes, the Random Forest and PCA-free Lasso perform similarly, with both continuing to improve as the number of training points grows. In contrast, the Lasso with 8 features selected via PCA does not substantially improve its MAE for larger training sizes - it effectively flattens out or only weakly improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why might this be the case?**: I believe the easiest to explain here first is the flattening of the Lasso with 8-feature PCA. When you limit the model to only using 8 features from PCA, you have effectively limited the complexity of the model and therefore the amount of variance in the data you can explain. In linear algebra terms, as you add more training points in this model, you are still limited to the same linearly independent \"bases\" and you therefore do not increase the ability of your model to 'span' your data as you add more training points. When you ease this limitation to 8 features and run the full Lasso-without-PCA model, you can now reduce your MAE by adding training points, as the complexity in the training data can be represented by a higher-dimensional model (more input data captures more information --> model works with more bases --> can 'span' more of the training data). This behavior is an example of the principle that \"larger datasets can benefit from more complex models\" (to directly quote one of the lecture notebooks). Lastly, the RF relies on building independent Decision Trees, each of which uses some subset of all available features. By providing more input data (rows) to an RF model, you expose your model to more information that it can use to more precisely discern where to \"branching\" should occur within each decision tree. Given a smaller subset of data, one could imagine that a RF model could entirely miss a dichotomous distinction between two groups of points because not enough members of each class within that dichotomy are sampled for that distinction to be useful as a discriminant within a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Comparing Feature Importance in LASSO and RF Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary to Question 2.1: Practice Computing Coefficients / Feature Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, I compute coefficients for Lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing things up to free memory\n",
    "# del train_data, test_data, train_desc, test_desc\n",
    "try: del pca8_lasso_predictions, nopca_lasso_predictions, rf_predictions\n",
    "except: print('Cells have probably been run out of order. Be careful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Lasso model again. See documentation in Q1 above. \n",
    "no_pca_model = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('lasso', LassoCV())\n",
    "])\n",
    "\n",
    "train_data, test_data, train_desc, test_desc = train_test_split(df, desc, train_size= 2000)\n",
    "nopca_lasso = no_pca_model.fit(train_desc, train_data['bandgap'])\n",
    "\n",
    "zip_obj = zip(nopca_lasso.named_steps['lasso'].coef_, desc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Lasso Coefficients (mainly as a check to see whether things worked). Taken from a lecture notebook.\n",
    "fig, ax = plt.subplots(figsize = (7,4))\n",
    "\n",
    "ax.bar(range(len(desc.columns)), nopca_lasso.named_steps['lasso'].coef_) ## See markdown cell below\n",
    "ax.set_xlim(ax.get_xlim())\n",
    "ax.plot(ax.get_xlim(), [0, 0], 'k--', lw= 2.)\n",
    "\n",
    "ax.set_ylabel(f'Correlation with Band Gap', fontsize = 14)\n",
    "ax.set_xlabel('Feature ID', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the **named_steps** call allows accessing the Lasso coefficients even though they are wrapped up in the Pipeline object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, I compute importances for the RandomForest (using the assigned feature scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel = RandomForestRegressor() ## rerunning this just for clarity of notebook\n",
    "rfmodel.fit(train_desc, train_data['bandgap'])\n",
    "feature_importances = pd.Series(rfmodel.feature_importances_, index=train_desc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have borrowed the cool trick above involving storing the features in the pandas Series objects from [this StackOverflow thread](https://stackoverflow.com/questions/44101458/random-forest-feature-importance-chart-using-python). This is super convenient because it keeps the names and importances bound to each other, without needing argsort. This could also be done with zip() but argsorting zip'd stuff in Python can be a pain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1: Comparing the top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "indices = np.argsort(nopca_lasso.named_steps['lasso'].coef_)[-10:]\n",
    "desc.columns[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "feature_importances.nlargest(10).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find in the cells above that the top 10 features for the models are completely different. This difference is perfectly expected: for one, we know that RF models will value features that are nonlinearly correlated for RF, whereas lasso will only consider a featrure important if it is linearly correlated with the regression target (see question 3). This alone likely has a huge effect in driving the observed difference in features. Secondly, because the most important features are found to be very variable between runs (see 2.3), the results are also not expected to be consistent on this basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2: Assessing Feature Self-Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here are super unstable between runs, so I have taken the results from one iteration and hardcoded them here. This means I have selected the top ten features from one of my runs, and I am slicing the desc dataframe and computing correlation from that slice. Starting with Lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_desc_lasso = desc[['fMF', 'mZagreb2', 'SMR_VSA7', 'ETA_epsilon_4', 'SlogP_VSA6', 'GATS2dv',\n",
    "       'VSA_EState3', 'ATSC2se', 'GATS1dv', 'NssO']]\n",
    "\n",
    "print(np.corrcoef(sliced_desc_lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_desc_rf = desc[['nBondsKD', 'AETA_beta', 'GATS1c', 'FCSP3', 'MATS1c', 'SdO',\n",
    "       'VSA_EState2', 'C2SP2', 'BCUTp-1l', 'ATSC2c']]\n",
    "\n",
    "print(np.corrcoef(sliced_desc_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: even though the numbers themselves are not really meaningful when viewed like this, we know IMMEDIATELY that there are strong correlations between our top 10 features. If they were uncorrelated, the results above would show all 0s, but instead, we observe that nearly all entries are nonzero = correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3: Refitting the Model on Newly Sampled Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question considers the reproducibility of the results from the models between successive runs. For sake of convenience of comparison, I will use the same RF model as before, but I ignore all previous results and generate two fresh train/test datasets below, and then will compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new test and training sets. Keeping 2000 steps so the number is the same as before\n",
    "train_data_newA, test_data_newA, train_desc_newA, test_desc_newA = train_test_split(df, desc, train_size= 2000)\n",
    "train_data_newB, test_data_newB, train_desc_newB, test_desc_newB = train_test_split(df, desc, train_size= 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keeping same model but fitting on new datasets; then plot bar plot of results\n",
    "\n",
    "plt.figure()\n",
    "fitA = rfmodel.fit(train_desc_newA, train_data_newA['bandgap'])\n",
    "feat_importances_A = pd.Series(rfmodel.feature_importances_, index=train_desc_newA.columns)\n",
    "feat_importances_A.nlargest(10).plot(kind='barh')\n",
    "plt.xlabel('Feature Importance')\n",
    "\n",
    "plt.figure()\n",
    "fitB = rfmodel.fit(train_desc_newB, train_data_newB['bandgap'])\n",
    "feat_importances_B = pd.Series(rfmodel.feature_importances_, index=train_desc_newB.columns)\n",
    "feat_importances_B.nlargest(10).plot(kind='barh')\n",
    "plt.xlabel('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I again acknowledge [here](https://stackoverflow.com/questions/44101458/random-forest-feature-importance-chart-using-python) for the Series trick and the plotting tactic shown above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the results above show that the most important features have minimal overlap between successive iterations of running the model. When I ran the model, it seemed like at most one feature appeared to show up in both test runs, whereas almost all other features changed. I therefore conclude that there is a high degree of randomness between successive runs of the RF model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4 (unofficially named)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe what these results mean for interpreting the features of machine learning models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of 2.1-2.3 tell us many things about interpreting the features of ML models. Firstly, as revealed by 2.1, they really tell us that discussions of \"best features\" are not universal/generally applicable to other models: they are model-specific. In other words, some features may be more important given one ML model, and those same features could be completely unimportant for another. Additionally, the results of 2.2 tell us that these features are (in general) distinctly not orthogonal: they are correlated with each other, and so we should be very careful and make sure to acknowledge this fact when deriving physical interpretations from our results. Lastly, from 2.3, we learn that these models have a degree of stochasticity/randomness to their results. Simply re-running the code can produce different results (I see this to be especially the case for RF), and so we might consider running many different iterations and noting which features are most important across an ensemble of runs. This idea of randomness/stochasticity in the model is something to keep in mind when we discuss the reproducibility of ML results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss the relative advantages of RandomForest versus Linear Regression versus Linear Regression with PCA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important difference between RF and the Lasso is that RF does not rely on assumption of linearity: for RF, features can be important so long as they help explain the variance in the regression target *in any way*. By contrast, features are only important in Lasso regression if they are linearly correlated with the regression target. This could be seen as an advantage for RF, but one could also imagine a situation where the assumption of linearity is appropriate or even desirable. Another advantage of RF is that you can run RF models when you have more columns/features than samples/rows in your data, which is impossible for linear regresson methods;  [(although in hindsight this fact is intutive, I originally learned this fact from this linked source while reading about RF models)](https://journals.sagepub.com/doi/full/10.1177/1536867X20909688). This is because RF models construct trees using only a subset of the input features at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to throw PCA into he mix. Linear Regression with PCA is the only one of the three that results in orthogonal features, which can be seen as an advantage of this method over the other two. The downside, though, is that these features are combinations of other features, and so are much harder to physically interpret compared to just the Linear Regression methods of the RF. Linear Regression with PCA also has the advantage of being faster: if you can pre-determine the most important features, you can get a much better ratio of (data used as input):(explained variance) than the Linear Regression method alone. However, both the RF and regular Linear Regression could be seen as advantaged in that they both have the *potential* to explain all the variance in the dependent variable, whereas Linear Regression with a limited numner of principal components can only explain *most* of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Molecular Fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, we are asked to:** Create a training set of 1000 entries. Train a k-Nearest Neighbors (kNN) regressor model using a Jaccard distance metric based on 128-length Morgan fingerprint with a radius of 3. Plot how the performance on the model (on a test set of 2000 entries) changes as you increase the number of neighbors used in kNN from 1 to $2^7$ by a factor of 2 each time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries, part 1: Reloading the data and Creating Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For safety, I reload the data below. This makes sense to do because the compute_morgan_fingerprints function below is already re-calling MolFromSmiles, so passing it our version with the 'mol' column already pre-computed makes things more confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('./qm9.json.gz', lines=True).sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train_data, test_data = train_test_split(data, train_size= 1000, test_size = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are directly taken from the 2_ml-with-fingerprints.ipynb lecture notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_morgan_fingerprints(smiles: str, fingerprint_length = 128, fingerprint_radius = 3):\n",
    "    \"\"\"Get Morgan Fingerprint of a specific SMILES string.\n",
    "    Adapted from: <https://github.com/google-research/google-research/blob/\n",
    "    dfac4178ccf521e8d6eae45f7b0a33a6a5b691ee/mol_dqn/chemgraph/dqn/deep_q_networks.py#L750>\n",
    "    Args:\n",
    "      graph (str): The molecule as a SMILES string\n",
    "      fingerprint_length (int): Bit-length of fingerprint\n",
    "      fingerprint_radius (int): Radius used to compute fingerprint\n",
    "    Returns:\n",
    "      np.array. shape = [hparams, fingerprint_length]. The Morgan fingerprint.\n",
    "    \"\"\"\n",
    "    # Parse the molecule\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Compute the fingerprint\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(\n",
    "        molecule, fingerprint_radius, fingerprint_length)\n",
    "    arr = np.zeros((1,), dtype=np.bool)\n",
    "\n",
    "    # ConvertToNumpyArray takes ~ 0.19 ms, while\n",
    "    # np.asarray takes ~ 4.69 ms\n",
    "    DataStructs.ConvertToNumpyArray(fingerprint, arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorganFingerprintTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Class that converts SMILES strings to fingerprint vectors\"\"\"\n",
    "    \n",
    "    def __init__(self, length: int = 256, radius: int = 4):\n",
    "        self.length = length\n",
    "        self.radius = radius\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Do need to do anything\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Compute the fingerprints\n",
    "        \n",
    "        Args:\n",
    "            X: List of SMILES strings\n",
    "        Returns:\n",
    "            Array of fingerprints\n",
    "        \"\"\"\n",
    "        \n",
    "        fing = [compute_morgan_fingerprints(m, self.length, self.radius) for m in X]\n",
    "        return np.vstack(fing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Building a simple kNN pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test/training datasets\n",
    "train_data, test_data = train_test_split(data, train_size = 1000, test_size  = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate model\n",
    "model = Pipeline([\n",
    "    ('fingerprint', MorganFingerprintTransformer()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=2, metric='jaccard', n_jobs=-1))  # n_jobs = -1 lets the model run all available processors\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model over a range of group sizes for the kNN\n",
    "\n",
    "results = [] \n",
    "for num_nn in tqdm(2**np.arange(8)): ## ranges from 1 to 128 neighbors\n",
    "    \n",
    "    ## Set model parameters. The first two are redundant with the default parameters I set for the \n",
    "    ## compute_morgan_fingerprints codeblock above, but I figured I would be safe.\n",
    "    model.set_params(fingerprint__length = 128, fingerprint__radius = 3, knn__n_neighbors = num_nn)\n",
    "    \n",
    "    # Train and test the model\n",
    "    model.fit(train_data['smiles_0'], train_data['bandgap'])\n",
    "    y_pred = model.predict(test_data['smiles_0'])\n",
    "\n",
    "\n",
    "    # Construct dictionary to temporarly results from each iteration\n",
    "    results.append({\n",
    "                'length': num_nn,\n",
    "                'r2_score': r2_score(test_data['bandgap'], y_pred),\n",
    "                'mae': mean_absolute_error(test_data['bandgap'], y_pred)\n",
    "                   })\n",
    "        \n",
    "# Store Results in pandas dataframe\n",
    "results = pd.DataFrame(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results that we stored in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,5))\n",
    "plt.plot(results['length'],results['mae'], color = 'mediumblue', lw = 2, marker = 'o')\n",
    "plt.xlabel('Number of Neighbors', fontsize = 16); _ = plt.xticks(fontsize = 14)\n",
    "plt.ylabel('MAE', fontsize = 16); _ = plt.yticks(fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of neighbors that minimizes the MAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**np.arange(8)[np.argmin(results['mae'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain why the MAE improves when increasing from 1 and then worsens as you increase past $2^4$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, to address the 'elephant in the room', I actually observe that the MAE exhibit a minimum at $2^3$ (= 8) neighbors, rather than 16. This does not particularly concern me, as everything looks correctly implemented to the best of my knowledge, so this may just be a matter of train/test sizes being different than when the assignment was initially written. The qualitative behavior seems to match, so I will proceed with an explanation in any case. <br/> <br/> As a bit of a recap, the key KNN parameter which I actually called num_nn in my code above) is the number of nearby datapoints put into the same group/class as a given datapoint, where \"nearby\" is defined in the multi-dimensional input space by the distance estimator we selected (here, the Jaccard distance). The regressor then effectively takes the mean of these groups in order to build up the relationship between the input variable and the target variable.. <br/> <br/> A consequence of this procedure is that when we set num_nn = 1, we are essentially telling the regressor to put the data into pairs based on their closeness, and take the mean based on that. This generally leads to a high sensitivity to noise (overfitting of a kind), since we are taking the mean of two numbers. This is what causes the high MAE at very low values of num_nn, since the estimate of the regression target can be biased by this noisy behavior. As you increase num_nn, there are more datapoints in each group, and so the means become slightly more robust. This beats down the noise and effectively \"smooths\" the relationship between the input variables and the regression target, leading to an improvement in the MAE. This improvement then stops at some fixed value at k, beyond which point the implicit \"smoothing\" is too agressive: you eventually have large enough groups of neighbors that you start losing information / underfitting complex behavior in your data. For that reason, the MAE starts becoming worse again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Implementing Recursive Feature Elimination into the kNN in the form of a RandomForest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment suggests us to use RFE to reduce the dimensionality of the data to 32 features in 4 steps. To do this, I use step = 0.25 in the RFE implementation below - I am removing 25% of features with each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN model with RFE. The nice thing about using sci-kit learn's Pipeline here is that the inputs\n",
    "# for one stage are automatically passed to the next, so we hardly need to think about how to manage\n",
    "# passing just the RFE-selected features to the KNN.\n",
    "\n",
    "knn_with_rfe = Pipeline([\n",
    "    ('fingerprint', MorganFingerprintTransformer()),\n",
    "    ('rfe',  RFE(estimator= random_forest_for_RFE, n_features_to_select= 32, step = 0.25)), \n",
    "    ('knn', KNeighborsRegressor(n_neighbors=2, metric='jaccard', n_jobs=-1))  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearly identical code to before, except now with new model.\n",
    "\n",
    "rfe_results = [] \n",
    "for num_nn in tqdm(2**np.arange(8)): ## ranges from 1 to 128 neighbors\n",
    "    \n",
    "    knn_with_rfe.set_params(fingerprint__length = 128, fingerprint__radius = 3, knn__n_neighbors = num_nn)\n",
    "    \n",
    "    # Train and test the model\n",
    "    knn_with_rfe.fit(train_data['smiles_0'], train_data['bandgap'])\n",
    "    y_pred = knn_with_rfe.predict(test_data['smiles_0'])\n",
    "\n",
    "\n",
    "    # Construct dictionary to temporarly results from each iteration\n",
    "    rfe_results.append({\n",
    "                'length': num_nn,\n",
    "                'r2_score': r2_score(test_data['bandgap'], y_pred),\n",
    "                'mae': mean_absolute_error(test_data['bandgap'], y_pred)\n",
    "                   })\n",
    "        \n",
    "# Store Results in pandas dataframe\n",
    "rfe_results = pd.DataFrame(rfe_results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparison of Results between the kNN with (blue) and without (red) the feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,5))\n",
    "plt.plot(rfe_results['length'],rfe_results['mae'], color = 'mediumblue', lw = 2, marker = 'o', label = 'With RFE')\n",
    "plt.plot(results['length'],results['mae'], color = 'red', lw = 2, marker = 'o', label = 'No RFE')\n",
    "\n",
    "plt.xlabel('Number of Neighbors', fontsize = 16); _ = plt.xticks(fontsize = 14)\n",
    "plt.ylabel('MAE', fontsize = 16); _ = plt.yticks(fontsize = 14)\n",
    "plt.legend(fontsize = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the RFE improves the MAE for all different values for num_nn. This improvement appears to be roughly constant as a function of num_nn. The intepretation for this behavior follows in Question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why would the model with the feature selection perform better?** **In general terms, explain the disadvantage of using a general-purpose distance metrics such as fingerprints and how must one must account for that.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the multi-dimensional kNN algorithm, many of the input features may be totally irrelevant. However, the kNN considers all features equally important(ie, it has no weighting). Therefore, a totally useless feature that has no physical correlation with a feature we may be interested in regressing to find (e.g, bandgap for the example at hand) may make it HARDER to identify groupings that we are trying to leverage within the kNN regression. In other words, extra unimportant features are weighted equally to important features, and therefore may make it harder for the kNN to group points together by adding \"noise\" to what might be a much stronger set of groupings based on the important features. Therfore, by using pre-emptive feature selection, we impose that all features used to define the groupings for the kNN regression have a meaningful relationship with the regression target, thereby reducing the ability of extraneous features to add extra \"noise\" that make defining kNN groupings hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to the second part of this question, my answer above reflects a central disadvantage of general-purpose distance metrics: namely, they do not weight features based on importance (and/or, viewed another way, cannot give you much insight about feature importance), and instead simply internally group points based on similarity. To account for that lack of importance-weighting, one needs to do something like we did in this question, such as first using a method like RandomForest or some form of PCA that DOES give you a sense of feature importance. By doing so, much of the obfuscating noise in a kNN regression can be mitigated by simply selecting the most important features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
